import os

from transformers import GPTNeoModel, GPTNeoForCausalLM, GPT2Tokenizer, GPTNeoConfig, AutoConfig
import torch
import npu
import random

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

# Globals setup in functions
gptj_model = None
model = None # CARP
tokenizer = None
skippable_tokens = []

# Globals accessed from functions
reviews = [
	"This kind of drags on.",
	"This is a bit too short.",
	"This is too cheery.",
	"This is really depressing.",
	"This is really exciting.",
	"This is boring.",
	"This ending leaves things too open.",
	"This ending feels abrupt.",
	"Could use more visual imagery.",
	"This is about bees.",
	"This story is generated by an AI.",
	"This story is weird.",
	"This would be good at show and tell."
]

carp_revision_prompt = "Rewrite Passage Based on Critique: "
gptj_revision_prompt = "Revise Passage As New Story: "
prompts = [
	{
	  "Passage": "Tom woke up in his apartment feeling tired on a rainy Saturday morning.",
	  "Critique": "Why was Tom tired?",
	  "Revision": "Rewrite Passage Based on Critique: Tom woke up in his apartment on a rainy Saturday morning, feeling tired from a late night of studying."
	},
	{
	  "Passage": "Tom woke up in his apartment feeling tired on a rainy Saturday morning.",
	  "Critique": "This is too much information at once.",
	  "Revision": "Rewrite Passage Based on Critique: Tom woke up in his apartment on a rainy Saturday morning."
	},
	{
	  "Passage": "Tom woke up in his apartment feeling tired on a rainy Saturday morning.",
	  "Critique": "Use a better adjective than tired.",
	  "Revision": "Rewrite Passage Based on Critique: Tom woke up in his apartment feeling groggy on a rainy Saturday morning."
	},
	{
	  "Passage": "Tom woke up in his apartment feeling tired on a rainy Saturday morning.",
	  "Critique": "This sentence is bland.",
	  "Revision": "Rewrite Passage Based on Critique: Tom woke to the sound of rain gently hitting his apartment window."
	},
	{
	  "Passage": "Tom woke up in his apartment feeling tired on a rainy Saturday morning.",
	  "Critique": "This sentence is boring. ",
	  "Revision": "Rewrite Passage Based on Critique: Tom woke to the sound "
	}
]

def setup_pretrained_models():
	# GPT-J 6B config
	config = AutoConfig.from_pretrained("EleutherAI/gpt-neo-2.7B")
	config.attention_layers = ["global"] * 28
	config.attention_types = [["global"], 28]
	config.num_layers = 28
	config.num_heads = 16
	config.hidden_size = 256 * config.num_heads
	config.vocab_size = 50400
	config.rotary = True
	config.rotary_dim = 64
	config.jax = True 

	try:
	    from collections.abc import MutableMapping
	except ImportError:
	    from collections import MutableMapping

	class Checkpoint(MutableMapping):
	    def __init__(self, checkpoint_entry="j6b_ckpt/m.pt"):
	        self.checkpoint = torch.load(checkpoint_entry, map_location="cpu")
	    def __len__(self):
	        return len(self.checkpoint)
	    def __getitem__(self, key):
	        return torch.load(self.checkpoint[key], map_location="cpu")
	    def __setitem__(self, key, value):
	        return
	    def __delitem__(self, key, value):
	        return
	    def keys(self):
	        return self.checkpoint.keys()
	    def __iter__(self):
	        for key in self.checkpoint:
	            yield (key, self.__getitem__(key))
	    def __copy__(self):
	        return Checkpoint()
	    def copy(self):
	        return Checkpoint()

	# the CPU held model *HAS* to be loaded first, if not when trying to load the model - which apparently uses the GPU while loading for some reason, maybe a default? -
	# will run out of memory to load stuff
	gptj_model = GPTNeoForCausalLM.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=Checkpoint(checkpoint_entry="gptj_model/j6b_ckpt/m.pt"))
	gptj_model.to("cpu")


	model = GPTNeoForCausalLM.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=Checkpoint(checkpoint_entry="j6b_ckpt/m.pt"))
	model = model.to("cuda")

def setup_tokenizer():
	tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-2.7B")
	tokenizer.add_tokens(["[QUOTE]"])

def establish_skippable_strings():
	skippable_strings = []
	# Setting up static words to skip
	for i in range(1, 30):
	  skippable_strings.append("_" * i);
	for i in range(2, 30):
	  skippable_strings.append("?" * i);
	for i in range(1, 30):
	  skippable_strings.append("Sam" + str(i) );
	for i in range(3, 30):
	  skippable_strings.append("." + str(i) );
	skippable_strings.append("\\n")
	skippable_strings.append("\n")

	quote_idx = len(tokenizer)-1
	skippable_tokens = [18559, 59, quote_idx]
	for ss in skippable_strings:
	  tokens = tokenizer(ss, add_prefix_space=True).input_ids
	  for t in tokens:
	    skippable_tokens.append(t)

	# Remove duplicates and wrap each element in a list
	skippable_tokens = list(dict.fromkeys(skippable_tokens))
	skippable_tokens = [ [st] for st in skippable_tokens]

def carp_critique(prompts):
	untokenized = ""
	for i in range(0, len(prompts)):
		untokenized += "Passage: " + prompts[i]["Passage"] + "\n\n"
		untokenized += "Critique: " + prompts[i]["Critique"] + "\n\n"
		#if i < len(prompts) - 1:
		untokenized +=  prompts[i]["Revision"] + "\n\n"

	toks = tokenizer.encode_plus(untokenized, return_tensors="pt").to("cuda")

	model.config.max_length=2048
	out = model.generate(
		**toks, 
		num_beams = 4, 
		repetition_penalty = 1.2, 
		no_repeat_ngram_size = 4, 
		early_stopping = True, 
		min_length = len(toks['input_ids'][0]) + 30, 
		bad_words_ids = skippable_tokens,
		num_return_sequences = 4
	)

	# TODO: use this once back and forth is proven out
	#print("OUTPUT:", len(out))
	#carp_out = []
	#for i in range(0, len(out)):
	#  carp_out.append(tokenizer.decode(out[i]))
	#print(carp_out)

	carp_output = tokenizer.decode(out[0])
	return carp_output

def gptj_revise(prompts):
	npu.api("NEURO API KEY", deployed=True)
	model_id = '60ca2a1e54f6ecb69867c72c'

	untokenized = ""
	for i in range(len(prompts) - 1, len(prompts)):
		untokenized += "Prompt: " + prompts[i]["Passage"] + "\n\n"
		untokenized += "Critique: " + prompts[i]["Critique"] + "\n\n"
		#if i < len(prompts) - 1:
		untokenized += prompts[i]["Revision"] + " "#+ "\n\n"

	tokens = tokenizer.encode_plus(untokenized, return_tensors="pt")


	kwargs = {
		'response_length': 50, # how many response tokens to generate
		'remove_input': False, # whether to return your input
		'num_beams': 4,
		'repetition_penalty': 1.2,
		'no_repeat_ngram_size': 4,
		'early_stopping': True,
		'bad_words_ids': skippable_tokens,
		'min_length': len(untokenized) + 30,
		'top_p': 0.9,
		'temperature': 0.8,
		# all params from the transformers library `generate` function are supported
	}

	gptj_output = npu.predict(model_id, [untokenized], kwargs)
	return gptj_output

def extract_carp_revision(prompts, output, carp_revision_prompt, gptj_revision_prompt, new_prompt_pos):
	output = output.replace("<|endoftext|>", "")

	rev_pos = output.rfind(carp_revision_prompt) + len(carp_revision_prompt)

	if rev_pos == -1:
		print("Revision not found during carp extraction")

	revision = output[rev_pos:]
	revision = revision.replace("\n", " ")

	prompts[-1]["Passage"] = " ".join(revision.split())
	prompts[-1]["Revision"] = gptj_revision_prompt + " ".join(revision.split(" ")[:new_prompt_pos]) + " "

	return prompts

def extract_revision(revision, prompts, carp_revision_prompt, gptj_revision_prompt, new_prompt_pos):
	generated_text = revision[0]["generated_text"]
	generated_text = generated_text.replace("rompt:", "")
	#rev_pos = generated_text.rfind(gptj_revision_prompt) + len(gptj_revision_prompt)
	rev_pos = generated_text.rfind("A:") + len("A:")

	if rev_pos == -1:
		print("Revision not found during GPT-J extraction")

	new_story = generated_text[rev_pos:].replace("\n", "")

	#print("GEN:",generated_text)
	#print("ORIGINAL:",prompts[-1]["Passage"])
	#print("NEW:", new_story)

	prompts[-1]["Passage"] = new_story
	prompts[-1]["Critique"] = random.choice(reviews)
	prompts[-1]["Revision"] = carp_revision_prompt + " ".join(new_story.split(" ")[:new_prompt_pos]) + " "

	return prompts

def main():
	setup_pretrained_models()
	setup_tokenizer()
	establish_skippable_strings()

	for i in range(0, 2):
		# CARP
		carp_output = carp_critique(prompts)
		prompts = extract_carp_revision(prompts, carp_output, carp_revision_prompt, gptj_revision_prompt, new_prompt_pos = 5)

		# GPT-J
		revision = gptj_revise(prompts)
		#print(revision)
		prompts = extract_revision(revision, prompts, carp_revision_prompt, gptj_revision_prompt, new_prompt_pos = 5)
		print("Iteration:", i, "Passage:", prompts[-1]["Passage"])
		print("Iteration:", i, "Critique:", prompts[-1]["Critique"])
		print("Iteration:", i, "Revision:", prompts[-1]["Revision"])

if __name__=="__main__":
    main()